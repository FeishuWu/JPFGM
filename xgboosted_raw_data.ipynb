{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#make sure you pip install sklearn_pandas (this is a very useful model)\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Briefing on xgboost\n",
    "\n",
    "XGBoost uses CART (Classification and Reggression tree)\n",
    " \n",
    "     - each leaf always contains a real-valued score (logistic or classification)\n",
    "     \n",
    "     - can later be converted into categories for classification problems\n",
    "     \n",
    "Boosting - ensemble meta-algorithm used to convert many weka learners into a strong learner by decreasing variance. \n",
    "\n",
    "Cross validation in XGBoost - is robust method to estimate the performance of a model on unseen data. (Different from the scikit-learn's cv). xgboost generates many non-overlapping train/test splits on training data then finally reports the average test set performance across all data splits\n",
    "\n",
    "DMatrix - makes data inot optimized data structure that xgb devs made to give the package its speedy attribute\n",
    "\n",
    "\n",
    "# Tuning the model\n",
    "\n",
    "boosting rounds (number of trees you build\n",
    "    early stopping helps by automatically selecting the number of boosting rounds for you within xgb.cv()\n",
    "    \n",
    "how does early boosting work?\n",
    " tests the xgb model after ever boostingg round aggainst a hold out set and stopping the creation of additional rounds (finishing training of the model early) if the hold out metric (e.g. \"rmse) does not improve for a given number of rounds. \n",
    " \n",
    " note largest number of boosting rounds is 50 then stoping does not occur. \n",
    " \n",
    "### Some Hyperparmaters explanation\n",
    "\n",
    "#### for trees\n",
    "    boosting rounds\n",
    "    learning rate (aka eta) larger value penalizes feature weights more strongly causing much stronger regularization\n",
    "    gamma: min loss reduction to create new tree split\n",
    "    lambda: L2 reg in leaf weights\n",
    "    alpha: L1 reg on leaf weights\n",
    "    lambda_bias: L2 reg on weights\n",
    "    max_depth: max depth per tree\n",
    "    subsample: % samples used per tree (underfitting if too low, overfitting if too high)\n",
    "    colsample_bytree: % features used per tree (smaller provides additional regularization, bigger u run into overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (307511, 122)\n",
      "Testing data shape:  (48744, 121)\n"
     ]
    }
   ],
   "source": [
    "path_to_kaggle_data = '~/Desktop/kaggle_competition/data/'\n",
    "\n",
    "# Training data\n",
    "app_train = pd.read_csv(path_to_kaggle_data + 'application_train.csv')\n",
    "print('Training data shape: ', app_train.shape)\n",
    "\n",
    "# Testing data features\n",
    "app_test = pd.read_csv(path_to_kaggle_data + 'application_test.csv')\n",
    "print('Testing data shape: ', app_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions (model_fit, grid_search, random_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, dtrain, dtest, filename_output, useTrainCV=True, encoding = 'ohe', cv_folds=5, early_stopping_rounds=50):\n",
    "    '''\n",
    "    This preprocesses the data to hot_encodes and optimizes the data, trains the datam and \n",
    "    xgboost predicts\n",
    "    \n",
    "    model= xgb.XGBClassifier() ; here we use a binary classifier\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = dtrain['SK_ID_CURR']\n",
    "    test_ids = dtest['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = dtrain['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    train_features = dtrain.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = dtest.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        train_features = pd.get_dummies(train_features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "    # Align the dataframes by the columns\n",
    "        train_features, test_features = train_features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "    # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(train_features):\n",
    "            if train_features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                train_features[col] = label_encoder.fit_transform(np.array(train_features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', train_features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    features = list(train_features.columns)\n",
    "    \n",
    "    # Preprocess data for xgb specifications\n",
    "    if useTrainCV:\n",
    "        xgb_params = model.get_xgb_params()\n",
    "        xgb_train = xgb.DMatrix(train_features.values, label=labels.values)\n",
    "        xgb_test = xgb.DMatrix(test_features.values)\n",
    "        cv_result = xgb.cv(xgb_params, xgb_train, num_boost_round= model.get_params()['n_estimators'], \n",
    "                          nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds) \n",
    "                          #show_progress=False)\n",
    "        model.set_params(n_estimators=cv_result.shape[0])\n",
    "    \n",
    "    # Fit model to training set\n",
    "    model.fit(train_features, labels, eval_metric = 'auc')\n",
    "    \n",
    "    # Predicting training set\n",
    "    dtrain_pred = model.predict(train_features)\n",
    "    dtrain_predprob = model.predict_proba(train_features)[:,1]\n",
    "    \n",
    "    # Predicting test set\n",
    "    test_predictions = model.predict(test_features)\n",
    "    # Uncomment if you have test_labels\n",
    "    #test_predprob = model.predict_proba(test_features)[:,1]\n",
    "    \n",
    "    # Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(labels.values, dtrain_pred))\n",
    "    print (\"AUC Score: %f\" % metrics.roc_auc_score(labels, dtrain_predprob))\n",
    "    \n",
    "    # Make DataFrame for submission\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    submission.to_csv(filename_output, index = False)\n",
    "    '''\n",
    "    if the test set had labels then we can use this\n",
    "    \n",
    "    # Predicting testing data:\n",
    "    dtest['predprob'] = model.predict_proba(test_features)[:,1]\n",
    "    results = test_results.merge(dtest[['SK_ID_CURR,'predprob']], on='SK_ID_CURR')\n",
    "    print 'AUC Score (Test): %f' % metrics.roc_auc_score(test_labels, results['predprob'])\n",
    "    '''   \n",
    "  \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def get_important_feat(model, feat_names):\n",
    "    '''\n",
    "    This function is to extract the feature importance\n",
    "    '''\n",
    "    from numpy import array\n",
    "    imp_vals = model.booster().get_fscore()\n",
    "    imp_dict = {feat_names[i]:float(imp_vals.get('f'+str(i),0.)) for i in range(len(feat_names))}\n",
    "    total = array(imp_dict.values()).sum()\n",
    "    return {k:v/total for k,v in imp_dict.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First version of xgb with data as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (307511, 241)\n",
      "Testing Data Shape:  (48744, 241)\n",
      "\n",
      "Model Report\n",
      "Accuracy : 0.9193\n",
      "AUC Score: 0.736448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learnig_rate=0.1, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=10, n_jobs=1, nthread=4, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=27, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define version 1 xgb \n",
    "xgb_v1 = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                           learnig_rate=.1,\n",
    "                           n_estimators=10,\n",
    "                           min_child_weight=1,  # because high class imbalance\n",
    "                           max_depth=5,\n",
    "                           gamma=0,\n",
    "                           subsample=0.8,\n",
    "                           colsample_bytree=0.8,\n",
    "                           nthread=4,\n",
    "                           scale_pos_weight=1,   # because high class imbalance\n",
    "                           seed=27)\n",
    "\n",
    "# Call the model_fit\n",
    "model_fit(xgb_v1, app_train, app_test, filename_output='baseline_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    48744\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We got a problem\n",
    "\n",
    "df = pd.read_csv('baseline_xgb.csv')\n",
    "df['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: Imbalance of data\n",
    "\n",
    "So we have defaults represented by 1 accounting for only 8% of the training data. We need to resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(app_train[\"TARGET\"] == 1)/len(app_train[\"TARGET\"]) *100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geovani's attempt to impute lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = features.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = features.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = features.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature],Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
